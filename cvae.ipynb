{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AE / VAE / CVAE\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## imports and load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "PATH = 'data'\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-3\n",
    "# MOMENTUM = 0.9\n",
    "EPOCHS = 50\n",
    "LATENT_DIM = 2\n",
    "\n",
    "# modules = dir(torch)\n",
    "# print(modules)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root=PATH, train=True, download=True, transform=transform)\n",
    "test_data = torchvision.datasets.MNIST(root=PATH, train=False, download=True, transform=transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## check data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bild, _ = train_data[0]\n",
    "print(bild.shape)\n",
    "print(f'minvalue, maxvalue: {torch.min(bild).item()}, {torch.max(bild).item()}')\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "rows, col = 3, 3\n",
    "\n",
    "for i in range(1, rows*col +1):\n",
    "    im, label = train_data[i]\n",
    "    fig.add_subplot(rows, col, i)\n",
    "    plt.title(label)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(im.squeeze())\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autoencoder "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder, Decoder and Autoencoder classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.fc1 = nn.Linear(32*7*7, 2 * self.latent_dim)\n",
    "        self.fc2 = nn.Linear(2 * self.latent_dim, self.latent_dim)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1, 32*7*7)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(self.latent_dim, 2 * self.latent_dim)\n",
    "        self.fc2 = nn.Linear(2 * self.latent_dim, 32*7*7)\n",
    "        self.bn_fc = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=2, padding=0, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.t_conv2 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=2, padding=0, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc1(input)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, 32, 7, 7)\n",
    "        #x = self.bn_fc(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.t_conv1(x)\n",
    "        #x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.t_conv2(x)\n",
    "        #x = self.bn2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        \n",
    "    def forward(self, input): \n",
    "        \n",
    "        x = self.encoder(input)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## check that classes are working correctly"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoencoder = Autoencoder(LATENT_DIM)\n",
    "autoencoder = autoencoder.to(device)\n",
    "\n",
    "im, _ = train_data[0]\n",
    "im = im.to(device)\n",
    "\n",
    "y = autoencoder(im.unsqueeze(0))\n",
    "\n",
    "pix = y.squeeze().cpu().detach().numpy()\n",
    "\n",
    "print(pix.shape)\n",
    "plt.imshow(pix)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## define model / set up loss and optimizer $\\circ $ **model**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Autoencoder(LATENT_DIM)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# print(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#training_loss = []\n",
    "print('starting training...')\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "    \n",
    "    epoch_loss = 0.0 \n",
    "    for batch in train_dataloader:\n",
    "        im, _ = batch\n",
    "        im = im.to(device)\n",
    "        \n",
    "        output = model(im)\n",
    "        \n",
    "        loss = criterion(im, output)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item()*im.size(0)\n",
    "    \n",
    "    epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    #training_loss.append(epoch_loss)\n",
    "    print(f'epoch {epoch+1} loss: {epoch_loss:.3f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## testing the trained model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# im, label = test_data[0]\n",
    "# im = im.to(device)\n",
    "\n",
    "# y = model(im.unsqueeze(0))\n",
    "# pix = y.squeeze().cpu().detach().numpy()\n",
    "# im = im.squeeze().cpu().detach().numpy()\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(2, 5)\n",
    "# ax1.imshow(im)\n",
    "# ax1.axis('off')\n",
    "# ax2.imshow(pix)\n",
    "# ax2.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "fig, axa = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "\n",
    "for j in range(5): \n",
    "    pix = []\n",
    "    im, _ = test_data[np.random.randint(0, len(test_data))]\n",
    "    pix.append(im.squeeze().cpu().detach().numpy())\n",
    "    y = model(im.to(device).unsqueeze(0))\n",
    "    pix.append(y.squeeze().cpu().detach().numpy())\n",
    "    \n",
    "    for i in range(2):\n",
    "        axa[i][j].imshow(pix[i])\n",
    "        axa[i,j].axis('off')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variational Autoencoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VEncoder, VDecoder and VAutoencoder classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class VEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.fc_mean = nn.Linear(32*7*7, self.latent_dim)\n",
    "        self.fc_std = nn.Linear(32*7*7, self.latent_dim)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1, 32*7*7)\n",
    "        mu = self.fc_mean(x)\n",
    "        sigma = self.fc_std(x)\n",
    "        z = self.reparametrization(mu, sigma)\n",
    "        return z, mu, sigma\n",
    "    \n",
    "    def reparametrization(self, mu, sigma):\n",
    "        if self.training: \n",
    "            r = torch.normal(0, 1, size=sigma.size()).to(device)\n",
    "            z = mu + sigma * r\n",
    "        else: z = mu\n",
    "        return z\n",
    "    \n",
    "    \n",
    "class VDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.fc = nn.Linear(self.latent_dim, 32*7*7)\n",
    "        self.bn_fc = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=2, padding=0, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.t_conv2 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=2, padding=0, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc(input)\n",
    "        x = x.view(-1, 32, 7, 7)\n",
    "        x = self.bn_fc(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.t_conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.t_conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "class VAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.vencoder = VEncoder(latent_dim)\n",
    "        self.vdecoder = VDecoder(latent_dim)\n",
    "        \n",
    "    def forward(self, input): \n",
    "        \n",
    "        z, mu, sigma = self.vencoder(input)\n",
    "        x = self.vdecoder(z)\n",
    "        return x, mu, sigma"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## check that classes are working correctly"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "var_autoencoder = VAutoencoder(LATENT_DIM)\n",
    "var_autoencoder = var_autoencoder.to(device)\n",
    "\n",
    "im, _ = train_data[0]\n",
    "im = im.to(device)\n",
    "\n",
    "print(im.size())\n",
    "y, _, _ = var_autoencoder(im.unsqueeze(0))\n",
    "\n",
    "pix = y.squeeze().cpu().detach().numpy()\n",
    "\n",
    "print(pix.shape)\n",
    "plt.imshow(pix)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## define model / loss-function and optimizer    $\\circ$    **vae**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BETA = 1\n",
    "\n",
    "def loss_function(x, xhat, mu, sigma): \n",
    "\n",
    "    # print(torch.max(x), torch.min(x))\n",
    "    # print(torch.max(xhat), torch.min(xhat))\n",
    "\n",
    "    BCE = nn.functional.binary_cross_entropy(xhat, x, reduction='sum')\n",
    "    KLD = -BETA * 0.5 * torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "vae = VAutoencoder(LATENT_DIM)\n",
    "vae.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# print(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train vae"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_loss = []\n",
    "\n",
    "# number of epochs for training of the variational version \n",
    "NUM_EPOCHS = 50\n",
    "print('starting training vae...')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS): \n",
    "    \n",
    "    epoch_loss = 0.0 \n",
    "    for batch in train_dataloader:\n",
    "        im, _ = batch\n",
    "        im = im.to(device)\n",
    "        \n",
    "        output, mu, sigma = vae(im)\n",
    "        \n",
    "        loss = loss_function(im, output, mu, sigma)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item()*im.size(0)\n",
    "    \n",
    "    epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    training_loss.append(epoch_loss)\n",
    "    print(f'epoch {epoch+1} loss: {epoch_loss:.3f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## testing trained vae"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(range(1, NUM_EPOCHS + 1), training_loss, 'r.')\n",
    "plt.xticks(range(1, NUM_EPOCHS + 1 ))\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "fig, axa = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True, figsize=(12,4))\n",
    "\n",
    "for j in range(5): \n",
    "    pix = []\n",
    "    im, _ = test_data[np.random.randint(0, len(test_data))]\n",
    "    pix.append(im.squeeze().cpu().detach().numpy())\n",
    "    y = model(im.to(device).unsqueeze(0))\n",
    "    pix.append(y.squeeze().cpu().detach().numpy())\n",
    "    \n",
    "    for i in range(2):\n",
    "        axa[i][j].imshow(pix[i])\n",
    "        axa[i,j].axis('off')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## generating images with an instance of VDecoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axa = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(20, 2))\n",
    "\n",
    "for i in range(10):\n",
    "    z = np.random.randn(LATENT_DIM)*np.random.randint(1, 4)\n",
    "    z = torch.Tensor(z).to(device)\n",
    "    pix = vae.vdecoder(z)\n",
    "    axa[i].imshow(pix.squeeze().detach().cpu().numpy())\n",
    "    axa[i].axis('off')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## plotting latent distribution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PLOT_BATCHES = 30\n",
    "\n",
    "for i, (x, y) in enumerate(train_dataloader):\n",
    "        z, _, _ = vae.vencoder(x.to(device))\n",
    "        z = z.detach().cpu().numpy()\n",
    "        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')\n",
    "        if i > PLOT_BATCHES:\n",
    "                break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r0=(-3, 7)\n",
    "r1=(-5, 5)\n",
    "n=12\n",
    "\n",
    "w = 28\n",
    "img = np.zeros((n*w, n*w))\n",
    "\n",
    "for i, y in enumerate(np.linspace(*r1, n)):\n",
    "    for j, x in enumerate(np.linspace(*r0, n)):\n",
    "        z = torch.Tensor([[x, y]]).to(device)\n",
    "        x_hat = vae.vdecoder(z)\n",
    "        x_hat = x_hat.reshape(28, 28).detach().cpu().numpy()\n",
    "        img[i*w:(i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.imshow(img, extent=[*r0, *r1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conditional Variational Autoencoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CVEncoder, CVDecoder and CVAutoencoder classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CVEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.fc0 = nn.Linear(28*28+10, 28*28)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.fc_mean = nn.Linear(32*7*7, self.latent_dim)\n",
    "        self.fc_std = nn.Linear(32*7*7, self.latent_dim)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input, c):\n",
    "        # fuse sample with labels\n",
    "        x = self.fc0(torch.cat([input, c], 1))\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1, 32*7*7)\n",
    "        mu = self.fc_mean(x)\n",
    "        sigma = self.fc_std(x)\n",
    "        z = self.reparametrization(mu, sigma)\n",
    "        return z, mu, sigma\n",
    "    \n",
    "    def reparametrization(self, mu, sigma):\n",
    "        if self.training: \n",
    "            r = torch.normal(0, 1, size=sigma.size()).to(device)\n",
    "            z = mu + sigma * r\n",
    "        else: z = mu\n",
    "        return z\n",
    "    \n",
    "    \n",
    "class CVDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.fc0 = nn.Linear(self.latent_dim + 10, 16*7*7)\n",
    "        self.fc1 = nn.Linear(16*7*7, 32*7*7)\n",
    "        self.bn_fc = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.t_conv1 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=2, padding=0, stride=2)\n",
    "\n",
    "        self.t_conv2 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=2, padding=0, stride=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        x = self.fc0(torch.cat([z, c], 1))\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, 32, 7, 7)\n",
    "        x = self.bn_fc(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.t_conv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.t_conv2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "class CVAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.cvencoder = CVEncoder(latent_dim)\n",
    "        self.cvdecoder = CVDecoder(latent_dim)\n",
    "        \n",
    "    def forward(self, input, c): \n",
    "        x = input.view(-1, 28*28)\n",
    "        z, mu, sigma = self.cvencoder(x, c)\n",
    "        x = self.cvdecoder(z, c)\n",
    "        return x, mu, sigma"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## define model / loss-function and optimizer    $\\circ$    **cvae**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LEARNING_R = 1e-3\n",
    "BETA = 1\n",
    "\n",
    "def loss_function(x, xhat, mu, sigma): \n",
    "\n",
    "    # print(torch.max(x), torch.min(x))\n",
    "    # print(torch.max(xhat), torch.min(xhat))\n",
    "\n",
    "    BCE = nn.functional.binary_cross_entropy(xhat, x, reduction='sum')\n",
    "    KLD = -BETA * 0.5 * torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "cvae = CVAutoencoder()\n",
    "cvae.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=LEARNING_R)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train cvae"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_loss = []\n",
    "\n",
    "# number of epochs for training of the variational version \n",
    "N_EPOCHS = 3\n",
    "print('starting training cvae...')\n",
    "\n",
    "for epoch in range(N_EPOCHS): \n",
    "    \n",
    "    epoch_loss = 0.0 \n",
    "    for batch in train_dataloader:\n",
    "        im, label = batch\n",
    "        im, label = im.to(device), label.to(device)\n",
    "        y = torch.nn.functional.one_hot(label, num_classes=10)\n",
    "        output, mu, sigma = cvae(im, y)\n",
    "        \n",
    "        loss = loss_function(im, output, mu, sigma)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item()*im.size(0)\n",
    "    \n",
    "    epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    training_loss.append(epoch_loss)\n",
    "    print(f'epoch {epoch+1} loss: {epoch_loss:.3f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## generating images with CVDecoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with torch.no_grad():\n",
    "    c = torch.eye(10, 10).to(device)\n",
    "    latent_sample = torch.randn(10, cvae.latent_dim).to(device)\n",
    "    sample = cvae.cvdecoder(latent_sample, c).cpu()\n",
    "    \n",
    "    plt.imshow(sample[5].view(28, 28))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "classes = torch.Tensor(list(range(10))).to(device)\n",
    "y = torch.nn.functional.one_hot(classes.to(torch.int64), num_classes=10)\n",
    "z = torch.randn(10,1).to(device)\n",
    "\n",
    "gpix = cvae.cvdecoder(z, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20,2))\n",
    "for i in range(10): \n",
    "\n",
    "      plt.subplot(1, 10, i+1)\n",
    "      plt.axis('off')\n",
    "      plt.imshow(gpix[i].detach().squeeze().cpu().numpy())\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## plotting latent distribution & sample gird"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PLOT_BATCHES = 30\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    x, y = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    x = x.view(-1, 28*28)\n",
    "    y_hot = torch.nn.functional.one_hot(y, num_classes=10)\n",
    "    z, _, _ = cvae.cvencoder(x, y_hot)\n",
    "    z = z.cpu().detach().numpy()\n",
    "    plt.scatter(z[:, 0], y.cpu(), c=y.cpu(), cmap='tab10')\n",
    "    if i > PLOT_BATCHES:\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r0=(0, 9)\n",
    "r1=(-10, 10)\n",
    "n=10\n",
    "m=20\n",
    "\n",
    "w = 28\n",
    "img = np.zeros((n*w, m*w))\n",
    "\n",
    "for i, y in enumerate(np.linspace(*r0, n)):\n",
    "    for j, x in enumerate(np.linspace(*r1, m)):\n",
    "        y = torch.Tensor([y, ])\n",
    "        y = y.to(device)\n",
    "        y_hot = torch.nn.functional.one_hot(y.to(torch.int64), num_classes=10)\n",
    "        z = torch.Tensor(np.array([x])).to(device)\n",
    "        z = torch.unsqueeze(z, 0)\n",
    "        x_hat = cvae.cvdecoder(z, y_hot)\n",
    "        x_hat = x_hat.reshape(28, 28).detach().cpu().numpy()\n",
    "        img[i*w:(i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "\n",
    "plt.figure(figsize = (15,30))\n",
    "\n",
    "plt.imshow(img, extent=[*r1, 0, 10])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('island': conda)"
  },
  "interpreter": {
   "hash": "5caaa1bc9b15b6b32c3354080b8d80e1a2af5d12081a1c55dce2cf9505551bea"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}